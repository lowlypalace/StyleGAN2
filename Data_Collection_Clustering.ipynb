{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Collection Clustering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1MfoPIsu3qVlakZSHmZ948zZSp4FZbZjI",
      "authorship_tag": "ABX9TyNqJyspgUCsbUln/JOgl/uB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lowlypalace/StyleGAN2/blob/main/Data_Collection_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2y6HV_WKMWS",
        "cellView": "form",
        "outputId": "2cd9ce59-179c-4987-d629-de48057646cd"
      },
      "source": [
        "#@title Run Setup\n",
        "!apt-get install libmagic-dev\n",
        "!pip install python-magic\n",
        "!pip install simplejson"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libmagic-dev is already the newest version (1:5.32-2ubuntu0.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.7/dist-packages (0.4.24)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.7/dist-packages (3.17.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7q1YY72hVq1",
        "cellView": "form"
      },
      "source": [
        "#@title Search Query\n",
        "#@markdown Set the search query. Use underscore instead of spaces.\n",
        "key = 'abstract_art' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSTn3rX9vfy3",
        "cellView": "form"
      },
      "source": [
        "#@markdown Set base directory\n",
        "base = \"/content/drive/MyDrive/Data\" #@param {type:\"string\"}\n",
        "\n",
        "try:\n",
        "  os.makedirs(base)\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4faG_V5CpbXB"
      },
      "source": [
        "# Data Collection\n",
        "The Bing Search API allows us to collect images for the lyrics search terms from Bing Image Search.\n",
        "\n",
        "- To run, simply give the script an image search query `search_query`. It will search the query and download the images into the `out_dir` directory. The images are dropped in subdirectory named the same as `search_query` with underscores.\n",
        "- Set `num_images` to the total number of pictures to download\n",
        "- Set `group_size` to how many photos to search per page\n",
        "- Set `api_key` to your Bing Search API key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay0D4srPbs21",
        "cellView": "form"
      },
      "source": [
        "#@title ## Set Search Options\n",
        "from easydict import EasyDict as edict\n",
        "from requests import exceptions\n",
        "import argparse\n",
        "import requests\n",
        "from PIL import Image\n",
        "import os\n",
        "import magic\n",
        "\n",
        "def setup_search_options(key, **kwargs):\n",
        "  \n",
        "  search_query = key.replace(\"_\", \" \") #@param {type:\"raw\"}\n",
        "\n",
        "  #@markdown Set output directory\n",
        "  out_dir = os.path.join(base, \"datasets\", key) #@param {type:\"raw\"} \n",
        "  #@markdown Number of images to search for\n",
        "  num_images =  500#@param {type:\"integer\"}\n",
        "  #@markdown How many images to search per page\n",
        "  group_size = 50 #@param {type:\"integer\"}\n",
        "  #@markdown ---\n",
        "\n",
        "  #@markdown # Set API Settings\n",
        "  #@markdown Set the API key\n",
        "  api_key = \"\" #@param {type:\"string\"}\n",
        "  #@markdown Set the API url\n",
        "url = \"https://api.bing.microsoft.com/v7.0/images/search\" #@param {type:\"string\"}\n",
        "\n",
        "  headers = {\"Ocp-Apim-Subscription-Key\": api_key} # Set API headers\n",
        "  params = {\"q\": search_query, \"offset\": 0, \"count\": group_size} # Set API headers\n",
        "\n",
        "  args = edict(kwargs.get('search_options', None))\n",
        "  args.api_args = getattr(args, 'api_args', edict())\n",
        "\n",
        "  args.search_query = getattr(args, 'search_query', search_query)\n",
        "  args.num_images = getattr(args, 'num_images', num_images)\n",
        "  args.group_size = getattr(args, 'group_size', group_size)\n",
        "  args.api_args.api_key = getattr(args.api_args, 'api_key', api_key)\n",
        "  args.api_args.url = getattr(args.api_args, 'url', url)\n",
        "  args.api_args.headers = getattr(args.api_args, 'headers', headers)\n",
        "  args.api_args.params = getattr(args.api_args, 'params', params)\n",
        "\n",
        "  return args, out_dir\n",
        "\n",
        "def get_images(key, search_options, out_dir):\n",
        "  search_query = search_options.search_query\n",
        "  num_images = search_options.num_images\n",
        "  group_size = search_options.group_size\n",
        "  api_key = search_options.api_args.api_key\n",
        "  url = search_options.api_args.url\n",
        "  headers = search_options.api_args.headers\n",
        "  params = search_options.api_args.params\n",
        "\n",
        "  exceptions_list = {IOError, FileNotFoundError, exceptions.RequestException, exceptions.HTTPError, exceptions.ConnectionError,\n",
        "                exceptions.Timeout}\n",
        "\n",
        "  # Initialize the search\n",
        "  print(\"Searching Bing API for '{}'\".format(search_query))\n",
        "  search = requests.get(url, headers=headers, params=params)\n",
        "  search.raise_for_status()\n",
        "\n",
        "  results = search.json()\n",
        "  estNumResults = min(results[\"totalEstimatedMatches\"], num_images)\n",
        "  print(\"Found {} results for '{}'\".format(estNumResults, search_query))\n",
        "  print()\n",
        "\n",
        "  # Initialize the total number of images downloaded\n",
        "  images_count = 0\n",
        "\n",
        "  # Loop over the estimated number of results in group_size\n",
        "  for offset in range(0, estNumResults, group_size):\n",
        "      # Update the search parameters using the current offset\n",
        "      print(\"Making request for group {}-{} of {}...\".format(\n",
        "          offset, offset + group_size, estNumResults))\n",
        "      params[\"offset\"] = offset\n",
        "      search = requests.get(url, headers=headers, params=params)\n",
        "      search.raise_for_status()\n",
        "      results = search.json()\n",
        "      print(\"Saving images for group {}-{} of {}...\".format(\n",
        "          offset, offset + group_size, estNumResults))\n",
        "\n",
        "      # Loop over the results\n",
        "      for v in results[\"value\"]:\n",
        "          try:\n",
        "              # Make a request to download the image\n",
        "              # print(\"Fetching: {}\".format(v[\"contentUrl\"]))\n",
        "              r = requests.get(v[\"contentUrl\"], timeout=30)\n",
        "\n",
        "              # Save image\n",
        "              out_img = os.path.join(out_dir, f\"{str(images_count).zfill(5)}-{key}\")\n",
        "              with open(out_img, 'wb') as f:\n",
        "                  f.write(r.content)\n",
        "                  f.close()\n",
        "\n",
        "              # Check if an image is an actual image file\n",
        "              img_type = magic.from_file(out_img, mime=True)\n",
        "              if (img_type == 'image/jpeg'):\n",
        "                  os.rename(out_img, f\"{out_img}.jpg\")\n",
        "              elif (img_type == 'image/png'):\n",
        "                  os.rename(out_img, f\"{out_img}.png\")\n",
        "              else:\n",
        "                  # print(\"Deleting non-image file: {}\".format(out_img))\n",
        "                  os.remove(out_img)\n",
        "                  continue\n",
        "\n",
        "          # Catch any errors \n",
        "          except Exception as e:\n",
        "              print(e)\n",
        "              if type(e) in exceptions_list:\n",
        "                  # print(\"Skipping: {}\".format(v[\"contentUrl\"]))\n",
        "                  continue\n",
        "\n",
        "          # Update the counter\n",
        "          images_count += 1\n",
        "\n",
        "  print()\n",
        "  print(f\"Number of saved images: {images_count}\")\n",
        "\n",
        "def run_search(key, **kwargs):\n",
        "\n",
        "  search_options, out_dir = setup_search_options(key, **kwargs)\n",
        "\n",
        "  # Print options\n",
        "  print()\n",
        "  print('Search options:')\n",
        "  print(f'Search query:      {search_options.search_query}')\n",
        "  print(f'Number of images:  {search_options.num_images}')\n",
        "  print(f'Output directory:  {out_dir}')\n",
        "  print()\n",
        "\n",
        "  # Create output directory\n",
        "  try:\n",
        "    os.makedirs(out_dir)\n",
        "  except:\n",
        "    pass\n",
        "  \n",
        "  # Kick off image search\n",
        "  get_images(key, search_options, out_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6RymvCltHUz",
        "cellView": "form"
      },
      "source": [
        "#@title ## Run Search\n",
        "run_search(key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfvn_wOprzOP"
      },
      "source": [
        "# Image Clustering \n",
        "The clustering algorithm to detect the classes of pictures after they are collected is done. The algorithm uses K-Means for clustering and Tensorflow Keras applications with weights pre-trained on ImageNet for vectorization of the images.\n",
        "\n",
        "- Set `data_dir` to the directory with images for clustering.\n",
        "- Set `result_dir` as the output directory. The resulting clusters will be added to as subfolders named `cluster`.\n",
        "- Set the number of clusters `num_clusters` to be created by the clustering algorithm.\n",
        "- Select ImageNets (choose from: `\"Xception\"`, `\"VGG16\"`, `\"VGG19\"`, `\"ResNet50\"`, `\"InceptionV3\"`, `\"InceptionResNetV2\"`, `\"DenseNet\"`, `\"MobileNetV2\"`). You can also set it to `False` to not use any. Link: [https://keras.io/api/applications/](https://keras.io/api/applications/)\n",
        "\n",
        "\n",
        "- Set the `num_images` to the number of images to cluster. When set to `None`, all of the images in `data_dir` will be clustered."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oZOrIkjWnHu",
        "cellView": "form"
      },
      "source": [
        "#@title ## Set Clustering Options\n",
        "from easydict import EasyDict as edict\n",
        "\n",
        "def setup_clustering_options(key, **kwargs):\n",
        "\n",
        "  #@markdown Path with images for clustering\n",
        "  data_dir = os.path.join(base, \"datasets\", key) #@param {type:\"raw\"}\n",
        "  #@markdown Path of the output folder\n",
        "  result_dir = os.path.join(base, \"clustering\", key) #@param {type:\"raw\"}\n",
        "  #@markdown Number of clusters\n",
        "  num_clusters = 10 #@param {type:\"slider\", min:1, max:20, step:1} \n",
        "  #@markdown Number of examples to use, if \"None\" all of the images will be used\n",
        "  num_images = None #@param {type:\"raw\"}\n",
        "  #@markdown Set shape\n",
        "  shape = (224, 224) #@param {type:\"raw\"}\n",
        "  #@markdown Select ImageNet \n",
        "  use_imagenets = 'Xception' #@param [\"False\", \"'Xception'\", \"'VGG16'\", \"'ResNet50'\", \"'InceptionV3'\", \"'InceptionResNetV2'\", \"'DenseNet'\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "  if use_imagenets == False:\n",
        "    use_pca = False\n",
        "  else:\n",
        "    #@markdown Use PCA for dimentionaity reduction\n",
        "    use_pca = False #@param {type:\"boolean\"}\n",
        "\n",
        "  paths = os.listdir(data_dir)\n",
        "  if num_images == None:\n",
        "    num_images = len(paths)\n",
        "  else:\n",
        "    if num_images > len(paths):\n",
        "      num_images = len(paths)\n",
        "    else:\n",
        "      num_images = num_images\n",
        "\n",
        "  args = edict(kwargs.get('clustering_options', None))\n",
        "\n",
        "  args.num_clusters = getattr(args, 'num_clusters', num_clusters)\n",
        "  args.num_images = getattr(args, 'num_images', num_images)\n",
        "  args.use_imagenets = getattr(args, 'use_imagenets', use_imagenets)\n",
        "  args.use_pca = getattr(args, 'use_pca', use_pca)\n",
        "  args.shape = getattr(args, 'shape', shape)\n",
        "\n",
        "  return args, data_dir, result_dir\n",
        "\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "\n",
        "def load_images(num_images, shape, data_dir, result_dir):\n",
        "\n",
        "  paths = os.listdir(data_dir)\n",
        "  random.shuffle(paths)\n",
        "  image_paths = paths[:num_images]\n",
        "\n",
        "  images = []\n",
        "  for image in image_paths:\n",
        "    img = cv2.cvtColor(cv2.resize(cv2.imread(os.path.join(data_dir, image)), tuple(shape)), cv2.COLOR_BGR2RGB) / 255\n",
        "    images.append(img)\n",
        "  images = np.array(images)\n",
        "\n",
        "  return images, image_paths\n",
        "\n",
        "def imagenet_classify(use_imagenets, use_pca, images):\n",
        "  if use_imagenets == False:\n",
        "    images_out = images\n",
        "  else:\n",
        "    if use_imagenets.lower() == \"vgg16\":\n",
        "      model1 = tensorflow.keras.applications.vgg16.VGG16(include_top=False, weights=\"imagenet\", input_shape=(224,224,3))\n",
        "    elif use_imagenets.lower() == \"vgg19\":\n",
        "      model1 = tensorflow.keras.applications.vgg19.VGG19(include_top=False, weights=\"imagenet\", input_shape=(224,224,3))\n",
        "    elif use_imagenets.lower() == \"resnet50\":\n",
        "      model1 = tensorflow.keras.applications.resnet50.ResNet50(include_top=False, weights=\"imagenet\", input_shape=(224,224,3))\n",
        "    elif use_imagenets.lower() == \"xception\":\n",
        "      model1 = tensorflow.keras.applications.xception.Xception(include_top=False, weights='imagenet',input_shape=(224,224,3))\n",
        "    elif use_imagenets.lower() == \"inceptionv3\":\n",
        "      model1 = tensorflow.keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
        "    elif use_imagenets.lower() == \"inceptionresnetv2\":\n",
        "      model1 = tensorflow.keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
        "    elif use_imagenets.lower() == \"densenet\":\n",
        "      model1 = tensorflow.keras.applications.densenet.DenseNet201(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
        "    elif use_imagenets.lower() == \"mobilenetv2\":\n",
        "      model1 = tensorflow.keras.applications.mobilenetv2.MobileNetV2(input_shape=(224,224,3), alpha=1.0, depth_multiplier=1, include_top=False, weights='imagenet', pooling=None)\n",
        "    else:\n",
        "      print(\"\\n\\n Please use one of the following keras applications only [ \\\"vgg16\\\", \\\"vgg19\\\", \\\"resnet50\\\", \\\"xception\\\", \\\"inceptionv3\\\", \\\"inceptionresnetv2\\\", \\\"densenet\\\", \\\"mobilenetv2\\\" ] or False\")\n",
        "      sys.exit()\n",
        "\n",
        "    pred = model1.predict(images)\n",
        "    images_temp = pred.reshape(images.shape[0], -1)\n",
        "    if use_pca == False:\n",
        "      images_out = images_temp\n",
        "    else:\n",
        "      model2 = PCA(n_components=None, random_state=40)\n",
        "      model2.fit(images_temp)\n",
        "      images_out = model2\n",
        "    \n",
        "    return images_out\n",
        "\n",
        "def cluster_images(num_clusters, num_images, data_dir, result_dir, image_paths, images_out):\n",
        "  model = KMeans(n_clusters=num_clusters, n_jobs=-1, random_state=40)\n",
        "  model.fit(images_out)\n",
        "  predictions = model.predict(images_out)\n",
        "\n",
        "  # Copy images to result_dir\n",
        "  for i in range(num_images):\n",
        "    name, ext = os.path.splitext(image_paths[i])\n",
        "    img_name = f\"{name}-cluster{str(predictions[i])}{ext}\"\n",
        "    src = os.path.join(data_dir, image_paths[i])\n",
        "    dst = os.path.join(result_dir, f\"cluster{str(predictions[i])}\", img_name)\n",
        "    shutil.copy2(src, dst)\n",
        "\n",
        "def run_clustering(key, **kwargs):\n",
        "\n",
        "  clustering_options, data_dir, result_dir = setup_clustering_options(key, **kwargs)\n",
        "\n",
        "  # Print options\n",
        "  print()\n",
        "  print('Clustering options:')\n",
        "  print(f'Number of clusters:     {clustering_options.num_clusters}')\n",
        "  print(f'Number of images:       {clustering_options.num_images}')\n",
        "  print(f'Selected ImageNet:      {clustering_options.use_imagenets}')\n",
        "  print(f'Data directory:         {data_dir}')\n",
        "  print(f'Output directory:       {result_dir}')\n",
        "  print()\n",
        "\n",
        "  # Create output directory\n",
        "  try:\n",
        "    shutil.rmtree(result_dir)\n",
        "  except FileNotFoundError:\n",
        "    pass\n",
        "  os.makedirs(result_dir)\n",
        "  \n",
        "  # Create cluster subdirectories\n",
        "  for i in range(clustering_options.num_clusters):\n",
        "    os.makedirs(os.path.join(result_dir, f\"cluster{str(i)}\"))\n",
        "\n",
        "  # Load images\n",
        "  print(f\"Loading images from {data_dir}\")\n",
        "  images, image_paths = load_images(clustering_options.num_images, clustering_options.shape, data_dir, result_dir)\n",
        "  print(f\"{clustering_options.num_images} images have been loaded in a random order\")\n",
        "  print()\n",
        "\n",
        "  # Classify with ImageNet\n",
        "  print(f\"Classifying images with ImageNet\")\n",
        "  print()\n",
        "  images_out = imagenet_classify(clustering_options.use_imagenets, clustering_options.use_pca, images)\n",
        "\n",
        "  # Kick off clustering\n",
        "  print(f\"Starting clustering.\")\n",
        "  cluster_images(clustering_options.num_clusters, clustering_options.num_images, data_dir, result_dir, image_paths, images_out)\n",
        "  print(f\"Successfully added images to {clustering_options.num_clusters} clusters\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTIDeMxJVNnf",
        "cellView": "form"
      },
      "source": [
        "#@title ## Run Clustering\n",
        "run_clustering(key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSU7FG4f1umb"
      },
      "source": [
        "# Merge Clusters\n",
        "This allows to merge selected cluster into one directory.\n",
        "\n",
        "- Set `clusters` to the list of clusters. For example, `clusters = [0, 1, 7]`.\n",
        "- Set `data_dir` to the data directory with clusters subfolders.\n",
        "- Set `out_dir` to the output directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjUtpWG_10P8",
        "cellView": "form"
      },
      "source": [
        "from easydict import EasyDict as edict\n",
        "import os\n",
        "\n",
        "def setup_merging_options(key, **kwargs):\n",
        "\n",
        "  #@markdown Set list of clusters\n",
        "  clusters = [0, 2, 3, 6, 7] #@param {type:\"raw\"}\n",
        "  \n",
        "  args = edict(kwargs.get('merging_options', None))\n",
        "  args.clusters = getattr(args, 'clusters', clusters)\n",
        "\n",
        "  #@markdown Set data directory\n",
        "  data_dir = os.path.join(base, \"clustering\", key) #@param {type:\"raw\"}\n",
        "  #@markdown Set output directory\n",
        "  out_dir = os.path.join(base, \"out\", key) #@param {type:\"raw\"}\n",
        "\n",
        "  return args, data_dir, out_dir\n",
        "\n",
        "def merge_clusters(key, **kwargs):\n",
        "\n",
        "  merging_options, data_dir, out_dir = setup_merging_options(key, **kwargs)\n",
        "  clusters = merging_options.clusters\n",
        "\n",
        "  print()\n",
        "  print('Merging options:')\n",
        "  print(f'Selected clusters:      {clusters}')\n",
        "  print(f'Data directory:         {data_dir}')\n",
        "  print(f'Output directory:       {out_dir}')\n",
        "  print()\n",
        "\n",
        "  if not clusters:\n",
        "    print(\"No clusters selected.\")\n",
        "  else:\n",
        "    # Create output directory\n",
        "    try:\n",
        "      os.makedirs(out_dir)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    # Copy files from a cluster\n",
        "    for cluster_num in clusters:\n",
        "      src = os.path.join(data_dir, f\"cluster{cluster_num}\")\n",
        "      src_files = os.listdir(src)\n",
        "\n",
        "      for file_name in src_files:\n",
        "        full_file_name = os.path.join(src, file_name)\n",
        "        if os.path.isfile(full_file_name):\n",
        "            shutil.copy(full_file_name, out_dir)\n",
        "            \n",
        "    # Get a number of files in the dataset\n",
        "    path, dirs, files = next(os.walk(out_dir))\n",
        "    file_count = len(files)\n",
        "    print(f\"Completed. The merged clusters have {file_count} images\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRS60l1eXr-q",
        "cellView": "form"
      },
      "source": [
        "#@title ## Merge Clusters\n",
        "merge_clusters(key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nARLwx3XfiZ"
      },
      "source": [
        "# Run Multiple Search Queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkLC-Ikiygip"
      },
      "source": [
        "- Set `data` to the json with search queries and clusters to merge. \n",
        "- To search images, set `search = True`.\n",
        "- To cluster images, set `cluster = True`.\n",
        "- To merge clusters, set `merge = True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZaYO-RBVMeA",
        "cellView": "form"
      },
      "source": [
        "#@title Edit JSON\n",
        "\n",
        "from easydict import EasyDict as edict\n",
        "import json\n",
        "\n",
        "def create_json():\n",
        "\n",
        "  out_dir = os.path.join(base, \"tmp\") # Set output directory\n",
        "\n",
        "  try:\n",
        "    os.makedirs(out_dir)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  # Set the search queries\n",
        "  data = \"\"\"\n",
        "{\n",
        "    \"car\": [\n",
        "        {\n",
        "            \"merging_options\": {\n",
        "                \"clusters\": [1,2]\n",
        "            },\n",
        "            \"clustering_options\" : {},\n",
        "            \"search_options\": {\n",
        "                \"num_images\": 50\n",
        "            }\n",
        "        }\n",
        "    ],\n",
        "    \"phone\": [\n",
        "        {\n",
        "            \"merging_options\": {\n",
        "                \"clusters\": [] \n",
        "            },\n",
        "            \"clustering_options\" : {},\n",
        "            \"search_options\": {\n",
        "                \"num_images\": 50\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  keys = edict(json.loads(data))\n",
        "\n",
        "  with open(os.path.join(out_dir, 'keys.json'), 'wt') as f:\n",
        "      json.dump(keys, f, sort_keys=True, indent=4)\n",
        "    \n",
        "  return keys\n",
        "\n",
        "keys = create_json()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM5w4xqmhCKK"
      },
      "source": [
        "The code snippet below allows to run multiple tasks in parallel making it faster to search, cluster and merge datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70DVzMuvloNp",
        "cellView": "form"
      },
      "source": [
        "#@title Run Tasks\n",
        "#@markdown Run multiple tasks in parallel\n",
        "from multiprocessing import Pool\n",
        "\n",
        "def log(key):\n",
        "\n",
        "  out_dir = out_dir = os.path.join(base, \"tmp\", \"logs\")\n",
        "\n",
        "  try:\n",
        "    os.makedirs(out_dir)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  sys.stdout = open(os.path.join(out_dir, f'{key}.txt'),'w')\n",
        "\n",
        "def main(key):\n",
        "  \n",
        "  #@markdown ### Save logs\n",
        "  log_out = True #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "  #@markdown ---\n",
        "\n",
        "  #@markdown ### Select tasks\n",
        "  search = False #@param {type:\"boolean\"}\n",
        "  cluster = False #@param {type:\"boolean\"}\n",
        "  merge = False #@param {type:\"boolean\"}\n",
        "  #@markdown ---\n",
        "\n",
        "  if log_out:\n",
        "    log(key)\n",
        "\n",
        "  for i in keys[key]:\n",
        "\n",
        "    if search:\n",
        "      search_options = i['search_options']\n",
        "      run_search(key, search_options = search_options)\n",
        "      \n",
        "    if cluster:\n",
        "      clustering_options = i['clustering_options']\n",
        "      run_clustering(key, clustering_options = clustering_options)\n",
        "\n",
        "    if merge:\n",
        "      merging_options = i['merging_options']\n",
        "      merge_clusters(key, merging_options = merging_options)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  x = [x for x in keys]\n",
        "      \n",
        "  with Pool(3) as p:\n",
        "      p.map(main, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k-ZJ99mhMCF"
      },
      "source": [
        "This code snippet runs sequentially. It's useful to use it with a few search queries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc3Uw2FqYlNa",
        "cellView": "form"
      },
      "source": [
        "#@title Run Tasks\n",
        "\n",
        "def main():\n",
        "\n",
        "  search = False #@param {type:\"boolean\"}\n",
        "  cluster = False #@param {type:\"boolean\"}\n",
        "  merge = False #@param {type:\"boolean\"}\n",
        "\n",
        "  for key in keys:\n",
        "    for i in keys[key]:\n",
        "\n",
        "      if search:\n",
        "        search_options = i['search_options']\n",
        "        run_search(key,\n",
        "                   num_images = search_options.num_images,\n",
        "                   group_size = search_options.group_size,\n",
        "                   )\n",
        "        \n",
        "      if cluster:\n",
        "        clustering_options = i['clustering_options']\n",
        "        run_clustering(key,\n",
        "                       num_clusters = clustering_options.num_clusters,\n",
        "                       use_imagenets = clustering_options.use_imagenets)\n",
        "\n",
        "      if merge:\n",
        "        merging_options = i['merging_options']\n",
        "        merge_clusters(key, merging_options.clusters)\n",
        "        \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQbNL7ACKNBR"
      },
      "source": [
        "# Zip Dataset\n",
        "Don't forget to clean data before zipping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-EfMBRvKMfb",
        "cellView": "form"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "def zipit():\n",
        "\n",
        "    base_dir = os.path.join(base, \"out\")\n",
        "\n",
        "    zip_name = \"dataset\" #@param\n",
        "\n",
        "    dir_names = ['dataset1', 'dataset2'] #@param {type:\"raw\"}\n",
        "\n",
        "    folders = []\n",
        "    for dir in dir_names:\n",
        "      path = os.path.join(base_dir, dir)\n",
        "      folders.append(path)\n",
        "\n",
        "    count_images = 0\n",
        "\n",
        "    zip_filename = os.path.join(base, \"zips\", f\"{zip_name}.zip\")\n",
        "\n",
        "    zip_file = zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED)\n",
        "\n",
        "    for folder in folders:\n",
        "        for dirpath, dirnames, filenames in os.walk(folder):\n",
        "            for filename in filenames:\n",
        "                count_images += 1\n",
        "                zip_file.write(\n",
        "                    os.path.join(dirpath, filename), filename)\n",
        "\n",
        "    zip_file.close()\n",
        "\n",
        "    print(f\"{count_images} images have been zipped as {zip_name}.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtMvhPxiSPKO"
      },
      "source": [
        "zipit()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}